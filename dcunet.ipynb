{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import copy\n",
    "\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# в sklearn не все гладко, чтобы в colab удобно выводить картинки \n",
    "# мы будем игнорировать warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "       \n",
    "DEVICE = torch.device('cuda' if train_on_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = SAMPLE_RATE * 64 // 1000 + 4\n",
    "HOP_LENGTH = SAMPLE_RATE * 16 // 1000 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разные режимы датасета \n",
    "DATA_MODES = ['train', 'test']\n",
    "# работаем на видеокарте\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с аудио, который их обрезает/паддит то заданной длины, применяет оконное преобразование Фурье,\n",
    "    нормализует и приводит к тензору.\n",
    "    \"\"\"\n",
    "    def __init__(self, noisy_files, clean_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.clean_files = sorted(clean_files)\n",
    "        self.labels = [path.parent.name for path in self.noisy_files]\n",
    "        \n",
    "        # параметры stft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # размер датасета\n",
    "        self.len_ = len(self.noisy_files)\n",
    "        \n",
    "        # будем обрезать/паддить waveform аудиофайлов до этого размера\n",
    "        self.max_len = 165000\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        return waveform\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
    "        x_clean = self.load_sample(self.clean_files[index])\n",
    "        x_noisy = self.load_sample(self.noisy_files[index])\n",
    "        \n",
    "        # padding/cutting\n",
    "        x_clean = self._prepare_sample(x_clean)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "        \n",
    "        # оконное преобразование Фурье\n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True)\n",
    "        x_clean_stft = torch.stft(input=x_clean, n_fft=self.n_fft, \n",
    "                                  hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return x_noisy_stft, x_clean_stft\n",
    "        \n",
    "    def _prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        current_len = waveform.shape[1]\n",
    "        \n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        output[0, -current_len:] = waveform[0, :self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NOISY_DIR = Path('/home/philipp/Projects/DCUnet/data/test/noisy_testset')\n",
    "TEST_CLEAN_DIR = Path('/home/philipp/Projects/DCUnet/data/test/clean_testset')\n",
    "\n",
    "# TRAIN_NOISY_DIR = Path('/home/philipp/Projects/DCUnet/data/train/noisy_trainset')\n",
    "# TRAIN_CLEAN_DIR = Path('/home/philipp/Projects/DCUnet/data/train/clean_trainset')\n",
    "\n",
    "TRAIN_NOISY_DIR = TEST_NOISY_DIR\n",
    "TRAIN_CLEAN_DIR = TEST_CLEAN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))[:100]\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))[:100]\n",
    "\n",
    "train_noisy_files = sorted(list(TRAIN_NOISY_DIR.rglob('*.wav')))[:100]\n",
    "train_clean_files = sorted(list(TRAIN_CLEAN_DIR.rglob('*.wav')))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_noisy_files, train_clean_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=1, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconvt(x)\n",
    "        normed = self.cbn(conved)\n",
    "        if not self.last_layer:\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = normed / torch.abs(normed)\n",
    "            m_mag = torch.tanh(torch.abs(normed))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCUnet10(nn.Module):\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # downsampling/encoding\n",
    "        self.downsample0 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45)\n",
    "        self.downsample1 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=45, out_channels=90)\n",
    "        self.downsample2 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90)\n",
    "        self.downsample3 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90)\n",
    "        self.downsample4 = Encoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90)\n",
    "        \n",
    "        # upsampling/decoding\n",
    "        self.upsample0 = Decoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90)\n",
    "        self.upsample1 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90)\n",
    "        self.upsample2 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90)\n",
    "        self.upsample3 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=180, out_channels=45)\n",
    "        self.upsample4 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=90, output_padding=(0,1),\n",
    "                                 out_channels=1, last_layer=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, is_istft=False):\n",
    "        # downsampling/encoding\n",
    "        d0 = self.downsample0(x)\n",
    "        d1 = self.downsample1(d0) \n",
    "        d2 = self.downsample2(d1)        \n",
    "        d3 = self.downsample3(d2)        \n",
    "        d4 = self.downsample4(d3)\n",
    "        \n",
    "        # upsampling/decoding\n",
    "        u0 = self.upsample0(d4)\n",
    "        c0 = torch.cat((u0, d3), dim=1)\n",
    "        \n",
    "        u1 = self.upsample1(c0)\n",
    "        c1 = torch.cat((u1, d2), dim=1)\n",
    "        \n",
    "        u2 = self.upsample2(c1)\n",
    "        c2 = torch.cat((u2, d1), dim=1)\n",
    "        \n",
    "        u3 = self.upsample3(c2)\n",
    "        c3 = torch.cat((u3, d0), dim=1)\n",
    "        \n",
    "        u4 = self.upsample4(c3)\n",
    "        \n",
    "        # u4 - the mask\n",
    "        output = u4 * x\n",
    "        if is_istft:\n",
    "            output = torchaudio.functional.istft(output, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcunet10 = DCUnet10(N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sdr_loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        num = (y_pred * y_true).mean()\n",
    "        den = torch.norm(y_pred) * torch.norm(y_true)\n",
    "        return (-1) * num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wsdr_loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.sdr = sdr_loss()\n",
    "    def forward(self, x, y_pred, y_true):\n",
    "        z_pred = x - y_pred\n",
    "        z_true = x - y_true\n",
    "        sdr_y = self.sdr(y_pred, y_true)\n",
    "        sdr_z = self.sdr(z_pred, z_true)\n",
    "        alpha = (torch.norm(y_true) ** 2) / ((torch.norm(y_true) ** 2) * (torch.norm(z_true) ** 2))\n",
    "        \n",
    "        return alpha * sdr_y + (1 - alpha) * sdr_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs, train_on_gpu=False):\n",
    "    net.train()\n",
    "    losses = []\n",
    "    if(train_on_gpu):\n",
    "        net = net.cuda()\n",
    "        \n",
    "    for e in tqdm(range(epochs)):\n",
    "        # batch loop\n",
    "        counter = 0\n",
    "        for noisy_x, clean_x in tqdm(train_loader):\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                noisy_x, clean_x = noisy_x.cuda(), clean_x.cuda()\n",
    "\n",
    "            # zero  gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            pred_x = net(noisy_x)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "            print(loss)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % 20 == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for noisy_val, clean_val in test_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        noisy_val, clean_val = noisy_val.cuda(), clean_val.cuda()\n",
    "\n",
    "                    pred_val = net(noisy_val)\n",
    "\n",
    "                    # calculate the loss\n",
    "                    val_loss = loss(n_val, pred_val, clean_val)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "      \n",
    "    scheduler.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = wsdr_loss()\n",
    "optimizer = torch.optim.Adam(dcunet10.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6.3210e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/100 [00:03<04:59,  3.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.6107e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/100 [00:05<04:49,  2.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.9302e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/100 [00:08<04:40,  2.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0415e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/100 [00:11<04:41,  2.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.9863e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 5/100 [00:14<04:33,  2.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.9556e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 6/100 [00:17<04:26,  2.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.5648e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 7/100 [00:20<04:31,  2.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6.3801e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 8/100 [00:23<04:40,  3.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1442e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▉         | 9/100 [00:26<04:41,  3.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.2000e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 10/100 [00:29<04:39,  3.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3022e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 11/100 [00:32<04:35,  3.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6.7683e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 12/100 [00:36<04:31,  3.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.4090e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 13/100 [00:39<04:27,  3.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1459e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 14/100 [00:42<04:26,  3.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.4401e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 15/100 [00:45<04:21,  3.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0991e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▌        | 16/100 [00:48<04:17,  3.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3447e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 17/100 [00:51<04:14,  3.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.8020e-07, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 18/100 [00:54<04:13,  3.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0819e-06, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 19/100 [00:57<04:10,  3.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2726e-06, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ls = train(dcunet10, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dcunet]",
   "language": "python",
   "name": "conda-env-dcunet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
